# spike_tab.py
import polars as pl
import streamlit as st
import plotly.graph_objects as go
from datetime import datetime
from dateutil.relativedelta import relativedelta
from typing import Optional

from dashboard.utils.analysis import perform_spike_detection, get_spike_time_series
from dashboard.utils.constants import ColumnNames
from dashboard.utils.ui_components import render_filter_summary_badge, render_spike_filter_summary, render_bookmark_manager


def show(filters=None, lf: pl.LazyFrame = None):
    """
    Spike Detection íƒ­

    Args:
        filters: SidebarManagerì—ì„œ ìƒì„±ëœ í•„í„° ë”•ì…”ë„ˆë¦¬
            - date_range: (start_date, end_date) íŠœí”Œ
            - as_of_month: ê¸°ì¤€ ì›” (ì˜ˆ: "2025-11")
            - window: ìœˆë„ìš° í¬ê¸° (1 ë˜ëŠ” 3)
            - min_c_recent: ìµœì†Œ ìµœê·¼ ì¼€ì´ìŠ¤ ìˆ˜
            - z_threshold: Z-score ì„ê³„ê°’
            - eps: Epsilon ê°’
            - alpha: ìœ ì˜ìˆ˜ì¤€
            - correction: ë‹¤ì¤‘ê²€ì • ë³´ì • ë°©ë²•
            - min_methods: ì•™ìƒë¸” ìµœì†Œ ë°©ë²• ìˆ˜
        lf: MAUDE ë°ì´í„° LazyFrame
    """
    from dashboard.utils.constants import DisplayNames

    st.title(DisplayNames.FULL_TITLE_SPIKE)

    if lf is None:
        st.warning("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    # í•„í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
    if filters is None:
        filters = {}

    # í•„í„° ê°’ ì¶”ì¶œ - ê¸‰ì¦ íƒì§€ íŒŒë¼ë¯¸í„°
    as_of_month = filters.get('as_of_month', '2025-11')
    window = filters.get('window', 1)
    min_c_recent = filters.get('min_c_recent', 20)
    z_threshold = filters.get('z_threshold', 2.0)
    eps = filters.get('eps', 0.1)
    alpha = filters.get('alpha', 0.05)
    correction = filters.get('correction', 'fdr_bh')
    min_methods = filters.get('min_methods', 2)

    # ê³µí†µ í•„í„° ì¶”ì¶œ
    manufacturers = filters.get("manufacturers", [])
    products = filters.get("products", [])
    devices = filters.get("devices", [])
    defect_types = filters.get("defect_types", [])
    clusters = filters.get("clusters", [])

    # ==================== ë¶ë§ˆí¬ ê´€ë¦¬ ====================
    render_bookmark_manager(
        tab_name="spike",
        current_filters=filters,
        filter_keys=[
            "as_of_month", "window", "min_c_recent", "z_threshold",
            "eps", "alpha", "correction", "min_methods",
            "manufacturers", "products", "devices", "defect_types", "clusters"
        ]
    )

    # ==================== í•„í„° ìš”ì•½ ë°°ì§€ (Spike ì „ìš©) ====================
    render_filter_summary_badge(
        manufacturers=manufacturers,
        products=products,
        devices=devices,
        defect_types=defect_types,
        clusters=clusters
    )
    
    render_spike_filter_summary(
        as_of_month=as_of_month,
        window=window,
        min_c_recent=min_c_recent,
        z_threshold=z_threshold,
        alpha=alpha,
        correction=correction,
        min_methods=min_methods
    )
    st.markdown("---")

    # ê³µí†µ í•„í„° ì ìš©
    from dashboard.utils.filter_helpers import apply_common_filters
    filtered_lf = apply_common_filters(
        lf,
        manufacturers=manufacturers,
        products=products,
        devices=devices,
        defect_types=defect_types,
        clusters=clusters
    )

    # ìŠ¤íŒŒì´í¬ íƒì§€ ìˆ˜í–‰ (í•„í„°ë§ëœ ë°ì´í„°ë¡œ ê³„ì‚°)
    with st.spinner("ê¸‰ì¦ íƒì§€ ë¶„ì„ ì¤‘..."):
        result_df = outlier_detect_check(
            lf=filtered_lf,
            window=window,
            min_c_recent=min_c_recent,
            z_threshold=z_threshold,
            eps=eps,
            alpha=alpha,
            correction=correction,
            min_methods=min_methods,
            month=as_of_month,
            manufacturers=tuple(manufacturers) if manufacturers else (),
            products=tuple(products) if products else ()
        )

    if result_df is None or len(result_df) == 0:
        st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê¸‰ì¦ í‚¤ì›Œë“œë§Œ í•„í„°ë§ (ì•™ìƒë¸” ê¸°ì¤€)
    spike_df = result_df.filter(pl.col("is_spike_ensemble") == True)

    # ========================================
    # ğŸ’¡ SECTION 0: í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (ìµœìƒë‹¨ ë°°ì¹˜) - terminology ê¸°ë°˜
    # ========================================
    from dashboard.utils.terminology import get_term_manager

    term = get_term_manager()
    st.subheader("ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")

    if len(spike_df) > 0:
        # 1ï¸âƒ£ ê°€ì¥ ìœ„í—˜í•œ ê¸‰ì¦ (3ê°œ ë°©ë²• ëª¨ë‘ ë™ì˜)
        critical_spikes = spike_df.filter(pl.col("n_methods") == 3).sort("ratio", descending=True)

        if len(critical_spikes) > 0:
            top_critical = critical_spikes.head(1)
            keyword = top_critical["keyword"][0]
            ratio = top_critical["ratio"][0]
            c_recent = top_critical["C_recent"][0]
            c_base = top_critical["C_base"][0]

            st.error(term.format_message('spike_critical',
                                        keyword=keyword,
                                        c_base=c_base,
                                        c_recent=c_recent,
                                        ratio=ratio))

        # 2ï¸âƒ£ ìƒˆë¡­ê²Œ ë“±ì¥í•œ ê¸‰ì¦ (ì´ì „ ê¸°ê°„ì—” ì—†ì—ˆë˜ í‚¤ì›Œë“œ)
        new_spikes = spike_df.filter(pl.col("C_base") < 5)  # ê¸°ì¤€ ê¸°ê°„ì— ê±°ì˜ ì—†ì—ˆë˜ í‚¤ì›Œë“œ
        if len(new_spikes) > 0:
            new_count = len(new_spikes)
            new_keywords = new_spikes.head(3)["keyword"].to_list()
            st.warning(term.format_message('spike_new',
                                          new_count=new_count,
                                          examples=', '.join(new_keywords)))

        # 3ï¸âƒ£ íŒ¨í„´ë³„ ìš”ì•½
        severe_count = len(spike_df.filter(pl.col("pattern") == "severe"))
        if severe_count > 0:
            st.warning(term.format_message('spike_severe_pattern', severe_count=severe_count))
        else:
            alert_count = len(spike_df.filter(pl.col("pattern") == "alert"))
            if alert_count > 0:
                st.info(term.format_message('spike_alert_pattern', alert_count=alert_count))
    else:
        st.success(term.messages.get('spike_none'))
        st.info(term.messages.get('spike_none_good'))

    st.markdown("---")

    # ========================================
    # ğŸš¨ SECTION 1: ê¸‰ì¦ íƒì§€ ìš”ì•½
    # ========================================
    st.subheader("ğŸš¨ ê¸‰ì¦ íƒì§€ ìš”ì•½")

    # ì£¼ìš” ë©”íŠ¸ë¦­
    col_main1, col_main2, col_main3 = st.columns([2, 2, 3])

    with col_main1:
        st.metric(
            label="âš ï¸ íƒì§€ëœ ê¸‰ì¦",
            value=f"{len(spike_df)}ê°œ",
            delta=f"ì „ì²´ {len(result_df)}ê°œ ì¤‘",
            help="ì•™ìƒë¸” ë°©ë²•ìœ¼ë¡œ íƒì§€ëœ ê¸‰ì¦ í‚¤ì›Œë“œ ìˆ˜"
        )

    with col_main2:
        if len(spike_df) > 0:
            avg_methods = spike_df["n_methods"].mean()
            st.metric(
                label="ğŸ“Š í‰ê·  íƒì§€ ë°©ë²• ìˆ˜",
                value=f"{avg_methods:.2f}ê°œ",
                help="Ratio/Z-score/Poisson ì¤‘ ëª‡ ê°œì˜ ë°©ë²•ì´ ê¸‰ì¦ìœ¼ë¡œ íŒì •í–ˆëŠ”ì§€"
            )
        else:
            st.metric(label="ğŸ“Š í‰ê·  íƒì§€ ë°©ë²• ìˆ˜", value="N/A")

    with col_main3:
        if len(spike_df) > 0:
            max_ratio_row = spike_df.sort("ratio", descending=True).head(1)
            max_keyword = max_ratio_row["keyword"][0]
            max_ratio = max_ratio_row["ratio"][0]
            st.metric(
                label="ğŸ”¥ ìµœëŒ€ ê¸‰ì¦ í‚¤ì›Œë“œ",
                value=max_keyword,
                delta=f"{max_ratio:.2f}x ì¦ê°€",
                help="ê¸°ì¤€ ê¸°ê°„ ëŒ€ë¹„ ê°€ì¥ ë§ì´ ì¦ê°€í•œ í‚¤ì›Œë“œ"
            )
        else:
            st.metric(label="ğŸ”¥ ìµœëŒ€ ê¸‰ì¦ í‚¤ì›Œë“œ", value="ì—†ìŒ")

    # íŒ¨í„´ë³„ ë¶„í¬
    st.markdown("**íŒ¨í„´ë³„ ë¶„í¬**")
    pattern_counts = result_df.group_by("pattern").agg(pl.len().alias("count")).sort("count", descending=True)

    col1, col2, col3, col4 = st.columns(4)
    pattern_map = {
        "severe": ("ğŸ”´ ì‹¬ê°", col1),
        "alert": ("ğŸŸ  ê²½ê³ ", col2),
        "attention": ("ğŸŸ¡ ì£¼ì˜", col3),
        "general": ("ğŸŸ¢ ì¼ë°˜", col4)
    }

    for pattern, (label, col) in pattern_map.items():
        count = pattern_counts.filter(pl.col("pattern") == pattern)
        count_val = count["count"][0] if len(count) > 0 else 0
        col.metric(label, count_val)

    st.markdown("---")

    # ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„ (12ê°œì›”)
    end_date = datetime.strptime(as_of_month, "%Y-%m")
    start_date = end_date - relativedelta(months=11)
    start_month = start_date.strftime("%Y-%m")

    # ========================================
    # ğŸ“ˆ SECTION 2: ì‹œê³„ì—´ ì°¨íŠ¸ (ì‹œê°í™”)
    # ========================================
    st.subheader("ğŸ“ˆ í‚¤ì›Œë“œ ë¹„ìœ¨ ì¶”ì´ (Anomaly Detection)")

    # ì „ì²´ í‚¤ì›Œë“œ ëª©ë¡ (ratio ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
    all_keywords = result_df.sort("ratio", descending=True)["keyword"].to_list()
    spike_keywords = spike_df.sort("ratio", descending=True)["keyword"].to_list() if len(spike_df) > 0 else []
    severe_keywords = result_df.filter(pl.col("pattern") == "severe").sort("ratio", descending=True)["keyword"].to_list()
    alert_keywords = result_df.filter(pl.col("pattern") == "alert").sort("ratio", descending=True)["keyword"].to_list()

    # # ğŸ” ë””ë²„ê·¸: ë‚ ì§œ-ì œì¡°ì‚¬-ì œí’ˆêµ°-í‚¤ì›Œë“œ ë§¤í•‘
    # st.write("### ğŸ” ë””ë²„ê·¸: ë‚ ì§œ-ì œì¡°ì‚¬-ì œí’ˆêµ°-í‚¤ì›Œë“œ ë§¤í•‘")
    # debug_mapping = (
    #     filtered_lf
    #     .select([
    #         pl.col("date_received").dt.truncate("1mo").alias("month"),
    #         pl.col(ColumnNames.MANUFACTURER).alias("manufacturer"),
    #         pl.col(ColumnNames.PRODUCT_CODE).alias("product"),
    #         pl.col(ColumnNames.DEFECT_TYPE).alias("keyword")
    #     ])
    #     .filter(pl.col("keyword").is_in(all_keywords[:20]))  # ìƒìœ„ 20ê°œ í‚¤ì›Œë“œë§Œ
    #     .group_by(["month", "manufacturer", "product", "keyword"])
    #     .agg(pl.len().alias("count"))
    #     .sort(["month", "keyword", "count"], descending=[True, False, True])
    #     .head(100)
    #     .collect()
    # )
    # st.write(f"**ë§¤í•‘ ë°ì´í„° (ìƒìœ„ 100í–‰)**:")
    # st.dataframe(debug_mapping, height=300)

    # ë¹ ë¥¸ ì„ íƒ ë²„íŠ¼
    st.markdown("**ğŸ”˜ ë¹ ë¥¸ ì„ íƒ**")
    col_btn1, col_btn2, col_btn3, col_btn4, col_btn5 = st.columns(5)

    # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™” (ì´ˆê¸°ê°’ë§Œ ì„¤ì •, default íŒŒë¼ë¯¸í„° ì‚¬ìš© ì•ˆ í•¨)
    if 'selected_keywords' not in st.session_state:
        st.session_state.selected_keywords = all_keywords[:min(5, len(all_keywords))]

    with col_btn1:
        severe_count = len(severe_keywords)
        if st.button(f"ğŸ”´ ì‹¬ê° ({severe_count})", use_container_width=True, help=f"ì‹¬ê° íŒ¨í„´ í‚¤ì›Œë“œ {severe_count}ê°œ ì¤‘ ìµœëŒ€ 10ê°œ ì„ íƒ"):
            st.session_state.selected_keywords = severe_keywords[:10]
            st.rerun()

    with col_btn2:
        alert_count = len(alert_keywords)
        if st.button(f"ğŸŸ  ê²½ê³  ({alert_count})", use_container_width=True, help=f"ê²½ê³  íŒ¨í„´ í‚¤ì›Œë“œ {alert_count}ê°œ ì¤‘ ìµœëŒ€ 10ê°œ ì„ íƒ"):
            st.session_state.selected_keywords = alert_keywords[:10]
            st.rerun()

    with col_btn3:
        spike_count = len(spike_keywords)
        if st.button(f"âš ï¸ ê¸‰ì¦ ({spike_count})", use_container_width=True, help=f"ê¸‰ì¦ìœ¼ë¡œ íƒì§€ëœ í‚¤ì›Œë“œ {spike_count}ê°œ ì¤‘ ìµœëŒ€ 10ê°œ ì„ íƒ"):
            st.session_state.selected_keywords = spike_keywords[:10]
            st.rerun()

    with col_btn4:
        if st.button("ğŸ” Top 10", use_container_width=True, help="ë¹„ìœ¨ ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ì„ íƒ"):
            st.session_state.selected_keywords = all_keywords[:10]
            st.rerun()

    with col_btn5:
        if st.button("ğŸ”„ ì´ˆê¸°í™”", use_container_width=True, help="ê¸°ë³¸ê°’(ìƒìœ„ 5ê°œ)ìœ¼ë¡œ ì´ˆê¸°í™”"):
            st.session_state.selected_keywords = all_keywords[:5]
            st.rerun()

    # í‚¤ì›Œë“œ ë©€í‹°ì…€ë ‰íŠ¸ (ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì™€ ì—°ë™, default ì œê±°í•˜ì—¬ ê²½ê³  ë°©ì§€)
    selected_keywords = st.multiselect(
        "ğŸ” í‘œì‹œí•  í‚¤ì›Œë“œ ì„ íƒ",
        options=all_keywords,
        key="selected_keywords",
        help="ì°¨íŠ¸ì— í‘œì‹œí•  í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”"
    )

    # ì„ íƒëœ í‚¤ì›Œë“œë¡œ ì‹œê³„ì—´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    if len(selected_keywords) > 0:
        ts_df_filtered = get_spike_time_series(
            _lf=filtered_lf,
            keywords=selected_keywords,
            start_month=start_month,
            end_month=as_of_month,
            window=window
        )

        if len(ts_df_filtered) > 0:
            fig = create_spike_chart(ts_df_filtered, z_threshold, as_of_month, window)
            st.plotly_chart(fig, width='stretch')
        else:
            st.info("ì„ íƒí•œ í‚¤ì›Œë“œì— ëŒ€í•œ ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ì°¨íŠ¸ë¥¼ í‘œì‹œí•  í‚¤ì›Œë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")

    # ========================================
    # ğŸ“‹ SECTION 3: ìƒì„¸ í…Œì´ë¸” (ìƒì„¸ ì •ë³´)
    # ========================================
    st.subheader("ğŸ“‹ ì „ì²´ ë¶„ì„ ê²°ê³¼")

    # í•„í„° ë„ì›€ë§
    with st.expander("â„¹ï¸ í•„í„° ì‚¬ìš© ë°©ë²•", expanded=False):
        st.markdown("""
        **ğŸ“Š íŒ¨í„´ í•„í„°**: í‘œì‹œí•  íŒ¨í„´ ìœ í˜• ì„ íƒ (ì‹¬ê°/ê²½ê³ /ì£¼ì˜/ì¼ë°˜)

        **âš ï¸ ê¸‰ì¦ë§Œ ì²´í¬ë°•ìŠ¤**:
        - âœ… ì²´í¬: ì•™ìƒë¸” ë°©ë²•ìœ¼ë¡œ **ê¸‰ì¦ íŒì •**ëœ í‚¤ì›Œë“œë§Œ í‘œì‹œ
        - â˜ ë¯¸ì²´í¬: ì„ íƒí•œ íŒ¨í„´ì˜ **ì „ì²´ í‚¤ì›Œë“œ** í‘œì‹œ (ê¸‰ì¦ ì•„ë‹Œ ê²ƒë„ í¬í•¨)

        **ğŸ”˜ ë¹ ë¥¸ í•„í„° í”„ë¦¬ì…‹**:
        - ğŸ”´ **Criticalë§Œ**: ì‹¬ê° íŒ¨í„´ + ê¸‰ì¦ë§Œ (ê°€ì¥ ìœ„í—˜í•œ í•­ëª©)
        - âš ï¸ **ì£¼ì˜ í•„ìš”**: ì‹¬ê°+ê²½ê³  íŒ¨í„´ ì „ì²´ (ê¸‰ì¦ ì•„ë‹Œ ê²ƒë„ í¬í•¨)
        - ğŸ“Š **ì „ì²´ ê¸‰ì¦**: ëª¨ë“  íŒ¨í„´ì˜ ê¸‰ì¦ë§Œ (íŒ¨í„´ ë¬´ê´€í•˜ê²Œ ê¸‰ì¦ íŒì •ëœ ê²ƒ)
        - ğŸ”„ **ì´ˆê¸°í™”**: ê¸°ë³¸ ì„¤ì • (ì‹¬ê°+ê²½ê³ +ì£¼ì˜ íŒ¨í„´ ì „ì²´)
        """)

    # ë¹ ë¥¸ í”„ë¦¬ì…‹ ë²„íŠ¼
    st.markdown("**ğŸ”˜ ë¹ ë¥¸ í•„í„°**")
    col_preset1, col_preset2, col_preset3, col_preset4 = st.columns(4)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'table_pattern_filter' not in st.session_state:
        st.session_state.table_pattern_filter = ["severe", "alert", "attention"]
    if 'table_spike_only' not in st.session_state:
        st.session_state.table_spike_only = False

    with col_preset1:
        if st.button("ğŸ”´ Criticalë§Œ", use_container_width=True, help="ì‹¬ê° íŒ¨í„´ + ê¸‰ì¦ë§Œ í‘œì‹œ"):
            st.session_state.table_pattern_filter = ["severe"]
            st.session_state.table_spike_only = True
            st.rerun()

    with col_preset2:
        if st.button("âš ï¸ ì£¼ì˜ í•„ìš”", use_container_width=True, help="ì‹¬ê° + ê²½ê³  íŒ¨í„´ ì „ì²´"):
            st.session_state.table_pattern_filter = ["severe", "alert"]
            st.session_state.table_spike_only = False
            st.rerun()

    with col_preset3:
        if st.button("ğŸ“Š ì „ì²´ ê¸‰ì¦", use_container_width=True, help="ëª¨ë“  íŒ¨í„´ì˜ ê¸‰ì¦ë§Œ"):
            st.session_state.table_pattern_filter = ["severe", "alert", "attention", "general"]
            st.session_state.table_spike_only = True
            st.rerun()

    with col_preset4:
        if st.button("ğŸ”„ ì´ˆê¸°í™”", use_container_width=True, help="ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë³µì›"):
            st.session_state.table_pattern_filter = ["severe", "alert", "attention"]
            st.session_state.table_spike_only = False
            st.rerun()

    # í…Œì´ë¸” í•„í„° (ì„¸ì…˜ ìƒíƒœ ì—°ë™)
    col_pattern, col_spike_only, col_topn = st.columns([3, 1, 1])
    with col_pattern:
        pattern_filter = st.multiselect(
            "ğŸ“Š íŒ¨í„´ í•„í„°",
            options=["severe", "alert", "attention", "general"],
            default=st.session_state.table_pattern_filter,
            format_func=lambda x: {
                "severe": "ğŸ”´ ì‹¬ê°",
                "alert": "ğŸŸ  ê²½ê³ ",
                "attention": "ğŸŸ¡ ì£¼ì˜",
                "general": "ğŸŸ¢ ì¼ë°˜"
            }[x],
            key="pattern_filter_table"
        )
        # ì„ íƒê°’ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state.table_pattern_filter = pattern_filter

    with col_spike_only:
        show_spike_only = st.checkbox(
            "âš ï¸ ê¸‰ì¦ë§Œ",
            value=st.session_state.table_spike_only,
            help="ì•™ìƒë¸” ê¸‰ì¦ìœ¼ë¡œ íŒì •ëœ í‚¤ì›Œë“œë§Œ í‘œì‹œ",
            key="spike_only_checkbox"
        )
        # ì„ íƒê°’ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state.table_spike_only = show_spike_only

    with col_topn:
        top_n_table = st.number_input(
            "í‘œì‹œ í–‰ ìˆ˜",
            min_value=10,
            max_value=200,
            value=50,
            step=10,
            key="top_n_table"
        )

    # í•„í„°ë§ëœ ê²°ê³¼ í…Œì´ë¸”
    filtered_result = result_df.filter(pl.col("pattern").is_in(pattern_filter))
    if show_spike_only:
        filtered_result = filtered_result.filter(pl.col("is_spike_ensemble") == True)

    display_all_df = prepare_spike_table(filtered_result.head(top_n_table))

    if len(display_all_df) > 0:
        st.dataframe(display_all_df, width='stretch', height=600)
    else:
        st.info("í•„í„° ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ========================================
    # ğŸ“¥ SECTION 4: ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    # ========================================
    st.markdown("---")
    col_download1, col_download2 = st.columns(2)

    with col_download1:
        st.markdown("**ğŸ“¥ ì „ì²´ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ**")
        csv_all = result_df.write_csv()
        st.download_button(
            label="ì „ì²´ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_all,
            file_name=f"spike_detection_all_{as_of_month}_w{window}.csv",
            mime="text/csv"
        )

    with col_download2:
        if len(spike_df) > 0:
            st.markdown("**ğŸ“¥ ê¸‰ì¦ë§Œ ë‹¤ìš´ë¡œë“œ**")
            csv_spike = spike_df.write_csv()
            st.download_button(
                label="ê¸‰ì¦ë§Œ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv_spike,
                file_name=f"spike_detection_spikes_{as_of_month}_w{window}.csv",
                mime="text/csv"
            )

def outlier_detect_check(
    lf: pl.LazyFrame,
    window: int = 1,
    min_c_recent: int = 20,
    z_threshold: float = 2.0,
    eps: float = 0.1,
    alpha: float = 0.05,
    correction: str = 'fdr_bh',
    min_methods: int = 2,
    month: str = "2025-11",
    manufacturers: tuple = None,
    products: tuple = None,
) -> Optional[pl.DataFrame]:
    """
    ìŠ¤íŒŒì´í¬ íƒì§€ ë¶„ì„ ìˆ˜í–‰

    Args:
        lf: MAUDE ë°ì´í„° LazyFrame (ì´ë¯¸ ê³µí†µ í•„í„° ì ìš©ë¨)
        window: ìœˆë„ìš° í¬ê¸° (1 ë˜ëŠ” 3)
        min_c_recent: ìµœì†Œ ìµœê·¼ ì¼€ì´ìŠ¤ ìˆ˜
        z_threshold: Z-score ì„ê³„ê°’
        eps: Epsilon ê°’ (z_log ê³„ì‚°ìš©)
        alpha: ìœ ì˜ìˆ˜ì¤€ (Poisson ê²€ì •ìš©)
        correction: ë‹¤ì¤‘ê²€ì • ë³´ì • ë°©ë²• ('bonferroni', 'sidak', 'fdr_bh', None)
        min_methods: ì•™ìƒë¸” ìŠ¤íŒŒì´í¬ íŒì • ìµœì†Œ ë°©ë²• ìˆ˜
        month: ê¸°ì¤€ ì›” (ì˜ˆ: "2025-11")
        manufacturers: ì œì¡°ì‚¬ í•„í„° (ìºì‹œ í‚¤ìš©)
        products: ì œí’ˆêµ° í•„í„° (ìºì‹œ í‚¤ìš©)

    Returns:
        ìŠ¤íŒŒì´í¬ íƒì§€ ê²°ê³¼ DataFrame
        ì»¬ëŸ¼: keyword, C_recent, C_base, ratio, z_log, score_pois,
              is_spike, is_spike_z, is_spike_p, n_methods, is_spike_ensemble, pattern
    """
    result_df = perform_spike_detection(
        _lf=lf,
        as_of_month=month,
        window=window,
        min_c_recent=min_c_recent,
        z_threshold=z_threshold,
        eps=eps,
        alpha=alpha,
        correction=correction,
        min_methods=min_methods,
        manufacturers=manufacturers,
        products=products,
    )

    return result_df


def create_spike_chart(
    ts_df: pl.DataFrame,
    z_threshold: float,
    as_of_month: str,
    window: int
) -> go.Figure:
    """
    ìŠ¤íŒŒì´í¬ ì‹œê³„ì—´ ì°¨íŠ¸ ìƒì„±

    Args:
        ts_df: ì‹œê³„ì—´ ë°ì´í„° (columns: month, keyword, count, ratio)
        z_threshold: Z-score ì„ê³„ê°’ (í‘œì‹œìš©)
        as_of_month: ê¸°ì¤€ ì›”
        window: ìœˆë„ìš° í¬ê¸°

    Returns:
        Plotly Figure ê°ì²´
    """
    import plotly.express as px
    from src import BaselineAggregator

    fig = go.Figure()

    # í‚¤ì›Œë“œë³„ë¡œ ë¼ì¸ ì¶”ê°€
    keywords = ts_df["keyword"].unique().to_list()

    # ë™ì  ìƒ‰ìƒ ìƒì„± (Plotlyì˜ qualitative ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì‚¬ìš©)
    n_colors = len(keywords)
    if n_colors <= 10:
        colors = px.colors.qualitative.Plotly
    elif n_colors <= 24:
        colors = px.colors.qualitative.Dark24
    else:
        # ë§ì€ í‚¤ì›Œë“œì˜ ê²½ìš° ìƒ‰ìƒ ë°˜ë³µ
        colors = px.colors.qualitative.Dark24 * ((n_colors // 24) + 1)

    for i, keyword in enumerate(keywords):
        keyword_data = ts_df.filter(pl.col("keyword") == keyword).sort("month")

        # ì›” ë¬¸ìì—´ì„ ì›” ì¤‘ê°„ ë‚ ì§œë¡œ ë³€í™˜ (ì˜ˆ: "2024-11" â†’ "2024-11-15")
        months = keyword_data["month"].to_list()
        month_mid_dates = [f"{m}-15" for m in months]

        fig.add_trace(go.Scatter(
            x=month_mid_dates,
            y=keyword_data["ratio"].to_list(),
            mode='lines+markers',
            name=keyword,
            line=dict(color=colors[i], width=2),
            marker=dict(size=6),
            hovertemplate='<b>%{fullData.name}</b><br>' +
                         'Month: %{x}<br>' +
                         'Ratio: %{y:.2f}x<br>' +
                         '<extra></extra>'
        ))

    # yì¶• ë²”ìœ„ ê³„ì‚°
    max_ratio = ts_df["ratio"].max() if len(ts_df) > 0 else z_threshold
    y_max = max(max_ratio, z_threshold) + 0.5

    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        title=f"ê¸‰ì¦ íƒì§€ - í‚¤ì›Œë“œ ë¹„ìœ¨ ì¶”ì´ (ìœˆë„ìš°: {window}ê°œì›”, ì„ê³„ê°’: {z_threshold:.2f}Ïƒ)",
        xaxis_title="ì›”",
        yaxis_title="ë¹„ìœ¨ (ë°°ìˆ˜)",
        yaxis=dict(range=[0, y_max]),
        hovermode='x unified',
        height=600,
        legend=dict(
            orientation="v",
            yanchor="top",
            y=1,
            xanchor="left",
            x=1.02
        ),
        margin=dict(l=50, r=150, t=80, b=50)
    )

    # z-score ì„ê³„ê°’ í‘œì‹œ
    fig.add_hline(
        y=z_threshold,
        line=dict(color="red", width=2, dash="dash"),
        annotation_text=f"Z-score ì„ê³„ê°’ ({z_threshold}Ïƒ)",
        annotation_position="right",
        annotation_font=dict(color="red", size=10)
    )

    # ê¸°ì¤€ êµ¬ê°„ê³¼ ë¹„êµ êµ¬ê°„ ì‹œê°ì  í‘œì‹œ
    if len(ts_df) > 0:
        all_months = sorted(ts_df["month"].unique().to_list())

        # BaselineAggregatorë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ê°„ ê³„ì‚°
        # BaselineAggregator ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë”ë¯¸)
        dummy_agg = BaselineAggregator(ts_df.lazy())

        # _get_window_months() ë©”ì„œë“œë¡œ êµ¬ê°„ ê³„ì‚°
        recent_months, baseline_months = dummy_agg._get_window_months(as_of_month, window)

        # ì‹¤ì œ ë°ì´í„°ì— ìˆëŠ” ì›”ë§Œ í•„í„°ë§
        baseline_months_in_data = [m for m in baseline_months if m in all_months]
        comparison_months_in_data = [m for m in recent_months if m in all_months]

        # ê¸°ì¤€ êµ¬ê°„ (íŒŒë€ìƒ‰)
        if baseline_months_in_data:
            baseline_sorted = sorted(baseline_months_in_data)

            # months[0] ~ months[-1] + 1m
            x0_month = baseline_sorted[0]
            last_date = datetime.strptime(baseline_sorted[-1], '%Y-%m')
            x1_date = last_date + relativedelta(months=1)
            x1_month = x1_date.strftime('%Y-%m')

            fig.add_vrect(
                x0=x0_month,
                x1=x1_month,
                fillcolor="lightblue",
                opacity=0.3,
                layer="below",
                line_width=0,
                annotation_text="ê¸°ì¤€" if window == 1 else "ê¸°ì¤€ êµ¬ê°„",
                annotation_position="top left",
                annotation_font=dict(size=10, color="blue")
            )

        # ë¹„êµ êµ¬ê°„ (ì£¼í™©ìƒ‰)
        if comparison_months_in_data:
            comparison_sorted = sorted(comparison_months_in_data)

            # months[0] ~ months[-1] + 1m
            x0_month = comparison_sorted[0]
            last_date = datetime.strptime(comparison_sorted[-1], '%Y-%m')
            x1_date = last_date + relativedelta(months=1)
            x1_month = x1_date.strftime('%Y-%m')

            fig.add_vrect(
                x0=x0_month,
                x1=x1_month,
                fillcolor="orange",
                opacity=0.3,
                layer="below",
                line_width=0,
                annotation_text="ë¹„êµ" if window == 1 else "ë¹„êµ êµ¬ê°„",
                annotation_position="top right",
                annotation_font=dict(size=10, color="darkorange")
            )

    return fig


def prepare_spike_table(spike_df: pl.DataFrame) -> pl.DataFrame:
    """
    ìŠ¤íŒŒì´í¬ í…Œì´ë¸” í‘œì‹œìš© ë°ì´í„° ì¤€ë¹„ (ì¤‘ìš” ì»¬ëŸ¼ ìš°ì„  ë°°ì¹˜)

    Args:
        spike_df: ìŠ¤íŒŒì´í¬ íƒì§€ ê²°ê³¼ DataFrame

    Returns:
        í‘œì‹œìš© DataFrame
    """
    # íŒ¨í„´ì— ì´ëª¨ì§€ ì¶”ê°€
    pattern_emoji = (
        pl.when(pl.col("pattern") == "severe").then(pl.lit("ğŸ”´ ì‹¬ê°"))
        .when(pl.col("pattern") == "alert").then(pl.lit("ğŸŸ  ê²½ê³ "))
        .when(pl.col("pattern") == "attention").then(pl.lit("ğŸŸ¡ ì£¼ì˜"))
        .otherwise(pl.lit("ğŸŸ¢ ì¼ë°˜"))
    )

    # ì¦ê° ê³„ì‚° (signed intë¡œ ëª…ì‹œì  ìºìŠ¤íŒ…í•˜ì—¬ ìŒìˆ˜ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)
    increase = (pl.col("C_recent").cast(pl.Int64) - pl.col("C_base").cast(pl.Int64))

    # ì»¬ëŸ¼ ìˆœì„œ: ì¤‘ìš”í•œ ì •ë³´ ìš°ì„  (í‚¤ì›Œë“œ â†’ íŒ¨í„´ â†’ ë¹„ìœ¨ â†’ ì¦ê° â†’ ë°©ë²• ìˆ˜ â†’ ìƒì„¸)
    display_df = spike_df.select([
        pl.col("keyword").alias("í‚¤ì›Œë“œ"),
        pattern_emoji.alias("íŒ¨í„´"),
        pl.col("ratio").round(2).alias("ë¹„ìœ¨ (ë°°ìˆ˜)"),
        pl.col("C_recent").alias("ìµœê·¼ ë³´ê³ ìˆ˜"),
        increase.alias("ì¦ê°"),
        pl.col("C_base").alias("ê¸°ì¤€ ë³´ê³ ìˆ˜"),
        pl.col("n_methods").alias("íƒì§€ë°©ë²•ìˆ˜"),
        pl.col("is_spike").alias("âœ“Ratio"),
        pl.col("is_spike_z").alias("âœ“Z-score"),
        pl.col("is_spike_p").alias("âœ“Poisson"),
    ])

    return display_df