# ============================================
# 저장소 설정 (S3, Snowflake)
# ============================================

# S3 설정
s3:
  enabled: true
  region: "us-east-1"
  bucket: "maude-data-pipeline"
  
  # # 인증 (환경변수 사용 권장)
  # access_key_id: "${AWS_ACCESS_KEY_ID}"
  # secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
  
  # 경로
  paths:
    bronze: "s3://maude-data-pipeline/bronze"
    silver: "s3://maude-data-pipeline/silver"
    gold: "s3://maude-data-pipeline/gold"
    temp: "s3://maude-data-pipeline/temp"
    logs: "s3://maude-data-pipeline/logs"
  
  # 파일 설정
  file_format:
    compression: "snappy"
    row_group_size: 100000
    
  # 업로드 설정
  upload:
    multipart_threshold: 100  # MB
    max_concurrency: 10

# Snowflake 설정
snowflake:
  # 연결 설정
  connection:
    timeout: 3600  # seconds (1시간)
    login_timeout: 60
    network_timeout: 600  # 10분
    client_session_keep_alive: true

  # 업로드 설정
  upload:
    auto_compress: true
    parallel: 4
    overwrite: true

  # COPY INTO 설정
  copy_into:
    file_format:
      type: "PARQUET"
      compression: "AUTO"

    on_error: "ABORT_STATEMENT"  # ABORT_STATEMENT, CONTINUE, SKIP_FILE
    purge: false  # 로드 후 스테이지에서 파일 삭제 여부
    force: false  # 이미 로드된 파일 재로드 여부

  # 테이블 설정
  tables:
    maude_clustered:
      name: "MAUDE_CLUSTERED"
      source_file: "maude_clustered.parquet"
      create_if_not_exists: true
      schema:
        # 자동으로 parquet 스키마에서 추론
        # 필요시 수동 정의 가능
        auto_infer: true

# 경로 설정
paths:
  local:
    data_file: "./data/silver/maude_clustered.parquet"

# Streamlit 대시보드
streamlit:
  host: "0.0.0.0"
  port: 8501
  
  data_sources:
    use_s3: true
    use_snowflake: true
    cache_ttl: 3600  # seconds