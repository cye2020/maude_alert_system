# ============================================
# 프로젝트 기본 설정
# ============================================

project:
  name: "maude-udi-pipeline"
  version: "1.0.0"
  description: "MAUDE & UDI Data Pipeline"

# 환경 설정
environment:
  env: "development"  # development, staging, production
  log_level: "INFO"
  timezone: "UTC"

# 경로 설정
paths:
  # 로컬 개발용
  local:
    root: "./data"
    bronze: "./data/bronze"
    silver: "./data/silver"
    gold: "./data/gold"
    temp: "./data/temp"
    logs: "./logs"
    
  # S3 프로덕션용 (실제 값은 storage.yaml에서)
  use_s3: false  # true면 S3 사용

# 데이터셋 이름
datasets:
  maude:
    name: "maude"
    # Bronze Layer - Raw Data
    bronze_file: "maude_raw.parquet"

    # Silver Layer - Cleaned & Transformed (3 stages)
    silver:
      stage1_basic_cleaning: "maude_basic_cleaned.parquet"      # 1차 전처리: 기본적인 데이터 정제
      stage2_text_processing: "maude_text_processed.parquet"    # 텍스트 전처리: 필드 정규화, 클리닝
      stage3_clustering: "maude_clustered.parquet"              # 클러스터링: 유사 케이스 그룹핑

    # Gold Layer - Business Aggregates (TBD)
    # 필터 조합이 많아 동적 집계 방식 검토 중
    # (년월별-제조사별-기기별-제품군별-defect_type별-cluster별)
    gold: null

  udi:
    name: "udi"
    bronze_file: "udi_raw.parquet"
    silver_file: "udi_cleaned.parquet"
    gold_file: "udi_matched.parquet"

# 로깅 설정
logging:
  version: 1
  disable_existing_loggers: false
  
  formatters:
    simple:
      format: "[%(asctime)s] %(levelname)s - %(message)s"
      datefmt: "%Y-%m-%d %H:%M:%S"
    
    detailed:
      format: "[%(asctime)s] %(levelname)s [%(name)s:%(funcName)s:%(lineno)s] - %(message)s"
      datefmt: "%Y-%m-%d %H:%M:%S"
  
  handlers:
    console:
      class: "logging.StreamHandler"
      level: "INFO"
      formatter: "simple"
      
    file:
      class: "logging.handlers.RotatingFileHandler"
      level: "DEBUG"
      formatter: "detailed"
      filename: "./logs/pipeline.log"
      maxBytes: 10485760  # 10MB
      backupCount: 5
  
  root:
    level: "INFO"
    handlers: ["console", "file"]

# 성능 설정
performance:
  # Polars 설정
  polars:
    n_threads: null  # null = 자동
    streaming: true
    
  # 청크 처리
  chunk_processing:
    enabled: true
    chunk_size: 100000
    max_memory_gb: 8
    
  # 병렬 처리
  parallel:
    max_workers: 4

# # 알림 설정
# notifications:
#   enabled: true
#   email:
#     enabled: true
#     recipients:
#       - "data-team@company.com"
#   slack:
#     enabled: false
#     webhook_url: "${SLACK_WEBHOOK_URL}"

# 테스트 모드
testing:
  use_sample_data: false
  sample_size: 10000