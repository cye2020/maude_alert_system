# ============================================
# 파이프라인 실행 설정
# ============================================

# 파이프라인 단계 정의
stages:
  - name: bronze
    enabled: true
    description: "원본 데이터 다운로드 및 저장"

  - name: silver
    enabled: true
    description: "초기 전처리 및 클린징"
    steps:
      - column_drop_1st
      - data_cleaning
      - deduplication

  - name: gold
    enabled: true
    description: "최종 전처리 및 매칭"
    steps:
      - udi_matching
      - quality_filtering
      - transformation
      - column_drop_2nd
      
# 체크포인트
checkpoints:
  enabled: true
  save_intermediate: true
  cleanup_on_success: false  # 성공시 중간 파일 삭제

# Airflow 설정
airflow:
  dags_folder: "./airflow/dags"
  
  # DAG 기본 설정
  default_args:
    owner: "data_team"
    depends_on_past: false
    email_on_failure: true
    email_on_retry: false
    retries: 2
    retry_delay_minutes: 5
  
  # 스케줄
  schedules:
    download_maude: "0 2 * * 1"      # 매주 월요일 02:00
    download_udi: "0 3 1 * *"        # 매월 1일 03:00
    silver_pipeline: "0 4 * * 1"     # 매주 월요일 04:00
    gold_pipeline: "0 6 * * 1"       # 매주 월요일 06:00

# 검증 설정
validation:
  enabled: false
  
  silver:
    min_rows: 1000
    max_duplicate_percentage: 1.0
    required_columns:
      - "report_number"
      - "date_of_event"
      
  gold:
    min_rows: 1000
    min_completeness: 0.70
    min_udi_match_rate: 0.50
    min_quality_score: 0.70

# 메타데이터 생성
metadata:
  enabled: true
  
  silver:
    generate:
      - "columns_dropped.json"
      - "cleaning_log.json"
      - "deduplication_stats.json"
      
  gold:
    generate:
      - "udi_matching_report.json"
      - "quality_report.json"
      - "schema.json"
      - "lineage.json"

# 에러 처리
error_handling:
  on_validation_failure: "alert"  # alert, stop, continue
  send_notification: true
  save_error_data: true
  max_consecutive_failures: 3