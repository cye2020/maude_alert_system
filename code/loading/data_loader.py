# ==================================================
# ë°ì´í„° ì ì¬ í•¨ìˆ˜
# ==================================================

# -----------------------------
# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# -----------------------------
import tempfile
from typing import List, Tuple, Union
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
import os
import json
import hashlib
from enum import Enum
import shutil

# -----------------------------
# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
# -----------------------------
import requests
from tqdm import tqdm
import pyarrow.parquet as pq
from pyspark.sql import SparkSession
import polars as pl
import pandas as pd

# -----------------------------
# ë¡œì»¬ ëª¨ë“ˆ
# -----------------------------
from .zip_streamer import ZipStreamer
from .flattener import Flattener
from .parquet_writer import ParquetWriter
from .schema_collector import SchemaCollector


# -----------------------------
# Dataset ì–´ëŒ‘í„°
# -----------------------------
class DatasetAdapter(Enum):
    SPARK = "spark"
    PANDAS = "pandas"
    POLARS = "polars"

class PolarsFrameType(Enum):
    LAZY_FRAME = 1
    DATA_FRAME = 2

class DataLoader:
    """FDA ë°ì´í„° ì „ì²´ ì ì¬ íŒŒì´í”„ë¼ì¸"""
    
    SEARCH_URL = 'https://api.fda.gov/download.json'
    
    def __init__(self, 
        start: int,
        end: int,
        output_file: str = 'output.parquet',
        schema_file: str = '.schema_cache.json',
        max_workers: int = 4,
        adapter: DatasetAdapter = DatasetAdapter.PANDAS
    ) -> None:
        self.start = start
        self.end = end
        self.output_file = output_file
        self.schema_file = schema_file
        self.max_workers = max_workers
        self.adapter = adapter
        self.urls = []
        self.schema_columns = []
    
    def search_download_url(self) -> List[str]:
        """ë‹¤ìš´ë¡œë“œ URL ëª©ë¡ ì¡°íšŒ"""
        response = requests.get(self.SEARCH_URL).json()
        partitions = response['results']['device']['event']['partitions']
        
        urls = []
        for item in partitions:
            first = item['display_name'].split()[0]
            if first.isdigit() and self.start <= int(first) <= self.end:
                urls.append(item["file"])
        return urls
    
    def _collect_schema_worker(self, url: str) -> Tuple[str, set, int]:
        """ì›Œì»¤ í•¨ìˆ˜: ë‹¨ì¼ URLì—ì„œ ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘"""
        collector = SchemaCollector()
        return collector.collect_from_url(url)
    
    def _collect_schema(self, skip: bool = False) -> List[str]:
        """Phase 1: ë³‘ë ¬ë¡œ ì „ì²´ ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘"""
        if skip and os.path.exists(self.schema_file):
            print(f"â™»ï¸  ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ë¡œë“œ: {self.schema_file}")
            with open(self.schema_file, 'r') as f:
                schema_columns = json.load(f)
            print(f"âœ… {len(schema_columns):,}ê°œ ì»¬ëŸ¼ ë¡œë“œ ì™„ë£Œ\n")
            return schema_columns
        
        print(f"\n=== Phase 1: ì „ì²´ ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘ (ë³‘ë ¬ {self.max_workers}ê°œ) ===\n")
        
        all_columns = set()
        total_records = 0
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self._collect_schema_worker, url): url 
                      for url in self.urls}
            
            for i, future in enumerate(tqdm(as_completed(futures), 
                                            total=len(self.urls), 
                                            desc="ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘"), 1):
                file_columns, record_count = future.result()
                all_columns.update(file_columns)
                total_records += record_count
        
        schema_columns = sorted(all_columns)
        
        # ìŠ¤í‚¤ë§ˆ ì €ì¥
        with open(self.schema_file, 'w') as f:
            json.dump(schema_columns, f)
        
        print(f"\nâœ… ì´ {total_records:,}ê°œ ë ˆì½”ë“œ, {len(schema_columns):,}ê°œ ê³ ìœ  ì»¬ëŸ¼ ë°œê²¬")
        print(f"ğŸ“ ìŠ¤í‚¤ë§ˆ ì €ì¥: {self.schema_file}\n")
        
        return schema_columns
    
    def _convert_url_to_temp_parquet(self, 
            url: str, 
            temp_dir: str
        ) -> Tuple[str, str, int]:
        """ì›Œì»¤ í•¨ìˆ˜: ë‹¨ì¼ URLì„ ì„ì‹œ Parquet íŒŒì¼ë¡œ ë³€í™˜"""
        try:
            # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„±
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            filename = f"{url_hash}_{url.split('/')[-1].replace('.zip', '.parquet')}"
            temp_file = os.path.join(temp_dir, filename)
            
            # ìŠ¤íŠ¸ë¦¬ë° ë³€í™˜
            streamer = ZipStreamer(url)
            flattener = Flattener()
            writer = ParquetWriter(self.schema_columns, temp_file)
            
            record_count = 0
            for record in streamer.stream_records():
                normalized = flattener.normalize(record, self.schema_columns)
                writer.write(normalized)
                record_count += 1
            
            writer.close()
            return temp_file, record_count
        
        except Exception as e:
            return None, 0
    
    def _merge_parquet_files(self, temp_files: List[str]) -> None:
        """ì„ì‹œ Parquet íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©"""
        print("\nğŸ“¦ Parquet íŒŒì¼ ë³‘í•© ì¤‘...")
        
        existing_files = [f for f in temp_files if os.path.exists(f)]
        
        if not existing_files:
            print("âŒ ë³‘í•©í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ParquetWriterë¡œ ë³‘í•©
        writer = ParquetWriter(self.schema_columns, self.output_file)
        
        for temp_file in tqdm(existing_files, desc="ë³‘í•©"):
            if os.path.exists(temp_file):
                table = pq.read_table(temp_file)
                writer.write_table(table)
                
                try:
                    os.remove(temp_file)
                except:
                    pass
        
        writer.close()
    
    def _convert_to_parquet(self) -> None:
        """Phase 2: ë³‘ë ¬ë¡œ Parquet ë³€í™˜"""
        print(f"=== Phase 2: Parquet ë³€í™˜ (ë³‘ë ¬ {self.max_workers}ê°œ) ===\n")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_dir = tempfile.mkdtemp(prefix='fda_parquet_')
        print(f"ğŸ“ ì„ì‹œ ë””ë ‰í† ë¦¬: {temp_dir}\n")
        
        total_records = 0
        temp_files = []
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._convert_url_to_temp_parquet, url, temp_dir): url 
                for url in self.urls
            }
            
            for future in tqdm(as_completed(futures), total=len(self.urls), desc="ë³€í™˜"):
                temp_file, record_count = future.result()
                
                if temp_file:
                    temp_files.append(temp_file)
                    total_records += record_count
                
        # ì„ì‹œ Parquet íŒŒì¼ë“¤ ë³‘í•©
        if temp_files:
            self._merge_parquet_files(temp_files)
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ
            try:
                shutil.rmtree(temp_dir)
            except:
                pass
            
            file_size_mb = os.path.getsize(self.output_file) / (1024 * 1024)
            print(f"\nâœ… ì™„ë£Œ! {total_records:,}ê°œ ë ˆì½”ë“œë¥¼ {self.output_file}ì— ì €ì¥")
            print(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
        else:
            print("\nâŒ ë³€í™˜ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def process(self, skip: bool = False):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ë°ì´í„° ë¡œë“œ"""
        start_time = time.time()
        
        # URL ìˆ˜ì§‘
        print("ğŸ” ë‹¤ìš´ë¡œë“œ URL ê²€ìƒ‰ ì¤‘...")
        self.urls = self.search_download_url()
        print(f"ì°¾ì€ URL: {len(self.urls)}ê°œ\n")
        
        if not self.urls:
            print("âŒ ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # Phase 1: ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘
        self.schema_columns = self._collect_schema(skip)
        
        # Phase 2: Parquet ë³€í™˜
        self._convert_to_parquet()
        
        total_time = time.time() - start_time
        print(f"\nâ±ï¸  ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")

    def load(self, adapter: Union[DatasetAdapter, str, None] = None, **kwargs):
        """ì–´ëŒ‘í„°ì— ë”°ë¼ Parquet íŒŒì¼ ë¡œë“œ"""
        if not os.path.exists(self.output_file):
            print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.output_file}")
            return None

        target_adapter = adapter or self.adapter or DatasetAdapter.PANDAS
        if isinstance(target_adapter, str):
            try:
                target_adapter = DatasetAdapter(target_adapter.lower())
            except ValueError as exc:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–´ëŒ‘í„°ì…ë‹ˆë‹¤: {adapter}") from exc

        print(f"\nğŸ“– {self.output_file} ë¡œë”© ì¤‘... (adapter={target_adapter.value})")

        if target_adapter == DatasetAdapter.PANDAS:
            return pd.read_parquet(self.output_file, **kwargs)
        if target_adapter == DatasetAdapter.POLARS:
            return pl.scan_parquet(self.output_file, **kwargs)
        if target_adapter == DatasetAdapter.SPARK:
            spark = SparkSession.builder.appName("DataLoader").getOrCreate()
            reader = spark.read
            if kwargs:
                reader = reader.options(**kwargs)
            return reader.parquet(self.output_file)

        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–´ëŒ‘í„°ì…ë‹ˆë‹¤: {target_adapter}")


# ============ ì‚¬ìš© ì˜ˆì‹œ ============
if __name__ == '__main__':
    loader = DataLoader(
        start=2020,
        end=2025,
        output_file='output.parquet',
        max_workers=4
    )

    loader.process(skip=False)
