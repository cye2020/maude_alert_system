from typing import List
import pyarrow as pa
import pyarrow.parquet as pq
from download import search_and_collect_json
from multiprocessing import Pool, cpu_count
from functools import partial
from tqdm import tqdm
import os

def flatten_dict(nested_dict, parent_key='', sep='_'):
    """ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ í‰íƒ„í™”"""
    items = []
    
    for k, v in nested_dict.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        elif isinstance(v, list) and v and isinstance(v[0], dict):
            for i, item in enumerate(v):
                items.extend(flatten_dict(item, f"{new_key}_{i}", sep=sep).items())
        else:
            items.append((new_key, v))
    
    return dict(items)

def clean_empty_arrays(obj):
    """ë¹ˆ ë¬¸ìì—´ë§Œ ìˆëŠ” ë°°ì—´ì„ Noneìœ¼ë¡œ ë³€í™˜"""
    if isinstance(obj, dict):
        return {k: clean_empty_arrays(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        if obj == [""]:
            return None
        return [clean_empty_arrays(item) for item in obj]
    elif obj == "":
        return None
    return obj

def process_record_for_columns(record):
    """ë‹¨ì¼ ë ˆì½”ë“œì—ì„œ ì»¬ëŸ¼ ì¶”ì¶œ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)"""
    cleaned = clean_empty_arrays(record)
    flattened = flatten_dict(cleaned)
    return set(flattened.keys())

def process_record_for_conversion(record, all_columns):
    """ë‹¨ì¼ ë ˆì½”ë“œë¥¼ ë³€í™˜ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)"""
    cleaned = clean_empty_arrays(record)
    flattened = flatten_dict(cleaned)
    
    # ì •ê·œí™”
    normalized = {}
    for col in all_columns:
        normalized[col] = flattened.get(col, None)
    
    # ë¬¸ìì—´ ë³€í™˜
    normalized = {k: (str(v) if v is not None else None) for k, v in normalized.items()}
    return normalized

def results_to_parquet_streaming_multiprocess(
    records_generator, 
    parquet_file, 
    chunk_size=5000,
    n_workers=None
):
    """
    ë©€í‹°í”„ë¡œì„¸ì‹±ê³¼ progress barë¥¼ ì‚¬ìš©í•œ Parquet ë³€í™˜
    
    Args:
        records_generator: ë ˆì½”ë“œ ì œë„ˆë ˆì´í„°
        parquet_file: ì¶œë ¥ íŒŒì¼ëª…
        chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ 5000)
        n_workers: ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: CPU ì½”ì–´ ìˆ˜ - 1)
    """
    if n_workers is None:
        max_workers = max(1, cpu_count() - 1)
        n_workers = min(16, max_workers)
    
    print(f"ğŸš€ ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©: {n_workers} workers")
    
    # ëª¨ë“  ë ˆì½”ë“œë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ (ì œë„ˆë ˆì´í„°ì´ë¯€ë¡œ í•œ ë²ˆë§Œ ìˆœíšŒ ê°€ëŠ¥)
    print("ğŸ“¥ ë ˆì½”ë“œ ë¡œë”© ì¤‘...")
    temp_records = list(tqdm(records_generator, desc="ë ˆì½”ë“œ ë¡œë”©"))
    total_records = len(temp_records)
    print(f"ì´ {total_records:,}ê°œ ë ˆì½”ë“œ ë¡œë“œ ì™„ë£Œ\n")
    
    # Pass 1: ëª¨ë“  ì»¬ëŸ¼ ìˆ˜ì§‘ (ë³‘ë ¬)
    print("=== Pass 1: ëª¨ë“  ì»¬ëŸ¼ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬) ===")
    all_columns = set()
    
    with Pool(n_workers) as pool:
        # chunksizeë¥¼ ì¡°ì •í•˜ì—¬ ì‘ì—… ë¶„ë°° íš¨ìœ¨ì„± í–¥ìƒ
        chunksize = max(1, total_records // (n_workers * 10))
        
        for columns_set in tqdm(
            pool.imap_unordered(process_record_for_columns, temp_records, chunksize=chunksize),
            total=total_records,
            desc="ì»¬ëŸ¼ ìŠ¤ìº”"
        ):
            all_columns.update(columns_set)
    
    all_columns = sorted(all_columns)
    print(f"âœ… ì´ {len(all_columns):,}ê°œ ê³ ìœ  ì»¬ëŸ¼ ë°œê²¬\n")
    
    # ìŠ¤í‚¤ë§ˆ ìƒì„±
    schema = pa.schema([(col, pa.string()) for col in all_columns])
    print(f"ğŸ“‹ ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ: {len(schema)} ì»¬ëŸ¼\n")
    
    # Pass 2: Parquet ë³€í™˜ (ë³‘ë ¬ ë³€í™˜ + ìˆœì°¨ ì“°ê¸°)
    print("=== Pass 2: Parquet ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬) ===")
    writer = pq.ParquetWriter(parquet_file, schema, compression='zstd')
    
    # ë¶€ë¶„ í•¨ìˆ˜ ìƒì„± (all_columnsë¥¼ ê³ ì •)
    process_func = partial(process_record_for_conversion, all_columns=all_columns)
    
    records_buffer = []
    total_processed = 0
    
    with Pool(n_workers) as pool:
        chunksize = max(1, total_records // (n_workers * 10))
        
        # imap_unorderedë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆœì„œ ë¬´ê´€í•˜ê²Œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
        for normalized_record in tqdm(
            pool.imap_unordered(process_func, temp_records, chunksize=chunksize),
            total=total_records,
            desc="ë³€í™˜ ë° ì €ì¥"
        ):
            records_buffer.append(normalized_record)
            
            # ì²­í¬ê°€ ì°¼ìœ¼ë©´ íŒŒì¼ì— ì“°ê¸°
            if len(records_buffer) >= chunk_size:
                table = pa.Table.from_pylist(records_buffer, schema=schema)
                writer.write_table(table)
                total_processed += len(records_buffer)
                records_buffer = []
    
    # ë‚¨ì€ ë ˆì½”ë“œ ì²˜ë¦¬
    if records_buffer:
        table = pa.Table.from_pylist(records_buffer, schema=schema)
        writer.write_table(table)
        total_processed += len(records_buffer)
    
    writer.close()
    print(f"\nâœ… ì™„ë£Œ! {total_processed:,}ê°œ ë ˆì½”ë“œë¥¼ {parquet_file}ì— ì €ì¥")
    
    # íŒŒì¼ í¬ê¸° ì¶œë ¥
    file_size_mb = os.path.getsize(parquet_file) / (1024 * 1024)
    print(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")

# ì‚¬ìš© ì˜ˆì‹œ
def record_generator(results: List[dict]):
    """dictì—ì„œ ë ˆì½”ë“œë¥¼ í•˜ë‚˜ì”© yield"""
    for record in results:
        yield record

if __name__=='__main__':
    start, end = 2024, 2024
    results = search_and_collect_json(start, end)
    
    # ë©€í‹°í”„ë¡œì„¸ìŠ¤ ë²„ì „ (ì¶”ì²œ)
    results_to_parquet_streaming_multiprocess(
        record_generator(results), 
        'output.parquet',
        chunk_size=5000,
        n_workers=None  # Noneì´ë©´ ìë™ìœ¼ë¡œ CPU ì½”ì–´ ìˆ˜ - 1
    )