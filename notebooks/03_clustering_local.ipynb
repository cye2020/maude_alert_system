{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAUDE 데이터 클러스터링 분석\n",
    "\n",
    "## ⚠️ GPU 필수 요구사항\n",
    "\n",
    "**이 노트북은 NVIDIA GPU가 필수입니다.**\n",
    "\n",
    "- RAPIDS (cuDF, cuML, cuPy) 라이브러리는 CUDA 지원 GPU에서만 작동합니다\n",
    "- GPU 메모리: 최소 8GB 이상 권장  \n",
    "- CUDA 버전: 11.x 또는 12.x 또는 13.x\n",
    "\n",
    "GPU가 없는 환경에서는 CPU 기반 대안을 사용하세요:\n",
    "- cuDF → Pandas\n",
    "- cuML UMAP → umap-learn  \n",
    "- cuML HDBSCAN → hdbscan (scikit-learn-contrib)\n",
    "\n",
    "---\n",
    "\n",
    "## 분석 개요\n",
    "\n",
    "MAUDE 의료기기 부작용 데이터를 다음 파이프라인으로 클러스터링합니다:\n",
    "\n",
    "1. **텍스트 임베딩**: SentenceTransformer (S-PubMedBert)\n",
    "2. **차원 축소**: UMAP (768D → 15D)\n",
    "3. **클러스터링**: HDBSCAN (밀도 기반)  \n",
    "4. **결과 분석**: 클러스터별 특성 파악\n",
    "\n",
    "---\n",
    "\n",
    "## 필요 패키지 설치\n",
    "\n",
    "```bash\n",
    "# RAPIDS 라이브러리 (CUDA 버전에 맞게 선택)\n",
    "# CUDA 11.x:\n",
    "pip install cudf-cu11 cuml-cu11 cupy-cuda11x\n",
    "\n",
    "# CUDA 12.x:\n",
    "pip install cudf-cu12 cuml-cu12 cupy-cuda12x\n",
    "\n",
    "# CUDA 13.x:\n",
    "pip install cudf-cu13 cuml-cu13 cupy-cuda13x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 표준 라이브러리\n",
    "# =====================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# =====================\n",
    "# 서드 파티 라이브러리\n",
    "# =====================\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuml\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from cuml import UMAP\n",
    "from cuml.cluster import HDBSCAN\n",
    "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import ast\n",
    "import joblib\n",
    "\n",
    "# =====================\n",
    "# 경로 설정\n",
    "# =====================\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "FONT_DIR = PROJECT_ROOT / 'font'\n",
    "\n",
    "# Python 내장 src 모듈과의 충돌 방지\n",
    "if str(PROJECT_ROOT) in sys.path:\n",
    "    sys.path.remove(str(PROJECT_ROOT))\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"✓ 라이브러리 임포트 완료\")\n",
    "print(f\"✓ 프로젝트 루트: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 한글 폰트 설정\n",
    "if platform.system() == 'Windows':\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "elif platform.system() == 'Darwin':  # macOS\n",
    "    plt.rcParams['font.family'] = 'AppleGothic'\n",
    "else:  # Linux\n",
    "    plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 로컬 폰트 지정\n",
    "font_path = FONT_DIR / 'PretendardVariable.ttf'\n",
    "fm.fontManager.addfont(str(font_path))\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = font_prop.get_name()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드 및 전처리\n",
    "\n",
    "### 2.1 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold 레이어 데이터 로드\n",
    "df = pl.read_parquet(DATA_DIR / \"silver\" / \"maude_text_processed.parquet\")\n",
    "\n",
    "print(f\"데이터 크기: {df.shape}\")\n",
    "print(f\"컬럼: {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 클러스터링용 텍스트 생성 (mdr_sntc)\n",
    "\n",
    "LLM이 추출한 4개 컬럼을 결합하여 임베딩용 텍스트를 생성합니다:\n",
    "- `patient_harm`: 환자 피해 여부\n",
    "- `problem_components`: 문제가 발생한 부품/컴포넌트\n",
    "- `defect_confirmed`: 결함 확인 여부\n",
    "- `defect_type`: 결함 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .with_columns(\n",
    "        # problem_components를 List에서 String으로 변환\n",
    "        pl.when(pl.col(\"problem_components\").is_not_null())\n",
    "        .then(pl.col(\"problem_components\").list.join(\", \"))\n",
    "        .otherwise(pl.lit(\"unknown\"))\n",
    "        .alias(\"problem_components_str\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        # mdr_sntc 생성: 4개 컬럼을 '. '로 결합\n",
    "        pl.concat_str([\n",
    "            pl.col(\"patient_harm\").cast(pl.String).fill_null(\"unknown\"),\n",
    "            pl.col(\"problem_components_str\"),\n",
    "            pl.col(\"defect_confirmed\").cast(pl.String).fill_null(\"unknown\"),\n",
    "            pl.col(\"defect_type\").cast(pl.String).fill_null(\"unknown\"),\n",
    "        ], separator=\". \").alias(\"mdr_sntc\")\n",
    "    )\n",
    "    .drop(\"problem_components_str\")\n",
    ")\n",
    "\n",
    "# 생성된 텍스트 예시\n",
    "print(\"\\n[mdr_sntc 예시]\")\n",
    "print(df.select(\"mdr_sntc\").head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 텍스트 임베딩 생성\n",
    "\n",
    "### S-PubMedBert 모델\n",
    "- 의료 도메인에 특화된 BERT 모델\n",
    "- 768차원 벡터로 텍스트 임베딩\n",
    "- 코사인 유사도 계산을 위해 정규화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# SentenceTransformer 모델 로드\n",
    "model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO')\n",
    "print(\"✓ 모델 로드 완료\")\n",
    "\n",
    "# 텍스트 리스트 추출\n",
    "texts = df.select(\"mdr_sntc\")['mdr_sntc'].to_list()\n",
    "print(f\"✓ 텍스트 개수: {len(texts):,}\")\n",
    "\n",
    "# 임베딩 생성\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=32,              # GPU 메모리에 맞게 조정\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True   # L2 정규화 (코사인 유사도 최적화)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ 임베딩 생성 완료\")\n",
    "print(f\"  - Shape: {embeddings.shape}\")\n",
    "print(f\"  - Dtype: {embeddings.dtype}\")\n",
    "\n",
    "# 임베딩 저장\n",
    "np.save(OUTPUT_DIR / \"embeddings.npy\", embeddings)\n",
    "print(f\"✓ 임베딩 저장: {OUTPUT_DIR / 'embeddings.npy'}\")\n",
    "\n",
    "# DataFrame에 임베딩 추가\n",
    "df = df.with_columns(\n",
    "    pl.Series(\"embeddings\", embeddings.tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. UMAP 차원 축소\n",
    "\n",
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "- 고차원 데이터의 구조를 보존하면서 저차원으로 축소\n",
    "- 768차원 → 15차원으로 축소\n",
    "- 클러스터링 성능과 계산 효율성 향상\n",
    "\n",
    "### 파라미터 설명\n",
    "- `n_neighbors=50`: 지역 구조를 파악할 이웃 개수 (클수록 전역 구조 중시)\n",
    "- `min_dist=0.0`: 저차원 공간에서 점들의 최소 거리 (0에 가까울수록 밀집)\n",
    "- `n_components=15`: 축소할 차원 수\n",
    "- `metric='cosine'`: 거리 측정 방식 (텍스트 임베딩에 적합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# UMAP 모델 초기화\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=50,\n",
    "    min_dist=0.0,\n",
    "    n_components=15,\n",
    "    metric='cosine',\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 차원 축소 수행 (GPU 가속)\n",
    "X = umap_model.fit_transform(embeddings)\n",
    "X = cp.asarray(X, dtype=cp.float32)\n",
    "\n",
    "print(f\"\\n✓ UMAP 차원 축소 완료\")\n",
    "print(f\"  - 입력 shape: {embeddings.shape}\")\n",
    "print(f\"  - 출력 shape: {X.shape}\")\n",
    "\n",
    "# UMAP 모델 및 결과 저장\n",
    "joblib.dump(umap_model, MODELS_DIR / \"umap_model.joblib\")\n",
    "np.save(OUTPUT_DIR / \"umap_X.npy\", cp.asnumpy(X))\n",
    "\n",
    "print(f\"✓ UMAP 모델 저장: {MODELS_DIR / 'umap_model.joblib'}\")\n",
    "print(f\"✓ 축소 결과 저장: {OUTPUT_DIR / 'umap_X.npy'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HDBSCAN 클러스터링\n",
    "\n",
    "### 5.1 평가 함수 정의\n",
    "\n",
    "클러스터링 품질을 측정하는 3가지 지표:\n",
    "- **Silhouette Score**: 클러스터 응집도와 분리도 (-1 ~ 1, 높을수록 좋음)\n",
    "- **Davies-Bouldin Index**: 클러스터 간 분리도 (0 ~ ∞, 낮을수록 좋음)\n",
    "- **Calinski-Harabasz Index**: 분산 비율 (0 ~ ∞, 높을수록 좋음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(X, labels, name):\n",
    "    \"\"\"\n",
    "    클러스터링 결과 평가 함수\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        특징 벡터\n",
    "    labels : array-like\n",
    "        클러스터 레이블 (-1은 noise)\n",
    "    name : str\n",
    "        클러스터링 방법 이름\n",
    "    \"\"\"\n",
    "    # Noise 제외\n",
    "    mask = labels != -1\n",
    "    \n",
    "    if len(set(labels[mask])) <= 1:\n",
    "        print(f\"{name}: 클러스터가 1개 이하입니다 (평가 불가)\")\n",
    "        return\n",
    "    \n",
    "    # 평가 지표 계산\n",
    "    sil = cython_silhouette_score(X[mask], labels[mask])\n",
    "    dbi = davies_bouldin_score(X[mask], labels[mask])\n",
    "    chi = calinski_harabasz_score(X[mask], labels[mask])\n",
    "    \n",
    "    print(f\"[{name}] Silhouette={sil:.3f}, DBI={dbi:.3f}, CHI={chi:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 HDBSCAN 하이퍼파라미터 그리드 서치\n",
    "\n",
    "HDBSCAN의 핵심 파라미터:\n",
    "- `min_cluster_size`: 클러스터로 간주할 최소 샘플 수 (클수록 큰 클러스터만 생성)\n",
    "- `min_samples`: 핵심 포인트 판별 기준 (클수록 보수적, noise 증가)\n",
    "- `metric`: 거리 측정 방식 (euclidean: UMAP 결과에 적합)\n",
    "\n",
    "목표: **Silhouette Score가 가장 높은** 파라미터 조합 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_until_k(X, target_k=18, \n",
    "                mcs_grid=(300, 500, 800, 1200, 2000, 3000, 5000, 8000, 12000),\n",
    "                ms_grid=(1, 3, 5, 10)):\n",
    "    \"\"\"\n",
    "    HDBSCAN 하이퍼파라미터 그리드 서치 (Silhouette Score 기준)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        특징 벡터 (UMAP 결과)\n",
    "    target_k : int\n",
    "        목표 클러스터 개수 상한 (이하의 결과만 고려)\n",
    "    mcs_grid : tuple\n",
    "        min_cluster_size 후보값들\n",
    "    ms_grid : tuple\n",
    "        min_samples 후보값들\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        최적 파라미터와 결과\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    X_np = cp.asnumpy(X) if hasattr(X, 'get') else X\n",
    "\n",
    "    for mcs in mcs_grid:\n",
    "        for ms in ms_grid:\n",
    "            # HDBSCAN 클러스터링 수행\n",
    "            hdb = HDBSCAN(\n",
    "                min_cluster_size=mcs,\n",
    "                min_samples=ms,\n",
    "                metric=\"euclidean\",\n",
    "                cluster_selection_method=\"eom\",  # Excess of Mass\n",
    "            )\n",
    "            labels = hdb.fit_predict(X)\n",
    "            labels_np = labels.get() if hasattr(labels, 'get') else labels\n",
    "\n",
    "            # 클러스터 개수 및 noise 비율 계산\n",
    "            k = len(np.unique(labels_np[labels_np != -1]))\n",
    "            noise = float((labels_np == -1).mean())\n",
    "\n",
    "            # 클러스터가 1개 이하이면 스킵\n",
    "            if k < 2:\n",
    "                print(f\"mcs={mcs:>5}, ms={ms:>2} -> k={k:>3}, noise={noise:.3f}, silhouette=N/A\")\n",
    "                continue\n",
    "\n",
    "            # Silhouette Score 계산 (noise 제외)\n",
    "            mask = labels_np != -1\n",
    "            sil = cython_silhouette_score(X_np[mask], labels_np[mask])\n",
    "\n",
    "            print(f\"mcs={mcs:>5}, ms={ms:>2} -> k={k:>3}, noise={noise:.3f}, silhouette={sil:.3f}\")\n",
    "\n",
    "            # 목표 클러스터 개수 이하이고, Silhouette Score가 가장 높은 것 선택\n",
    "            if k <= target_k:\n",
    "                if best is None or sil > best[\"silhouette\"]:\n",
    "                    best = {\n",
    "                        \"mcs\": mcs, \n",
    "                        \"ms\": ms, \n",
    "                        \"k\": k, \n",
    "                        \"noise\": noise, \n",
    "                        \"silhouette\": sil, \n",
    "                        \"labels\": labels, \n",
    "                        \"model\": hdb\n",
    "                    }\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 그리드 서치 실행\n",
    "print(\"HDBSCAN 하이퍼파라미터 그리드 서치 시작...\\n\")\n",
    "\n",
    "grid_lst = (12000, 16000, 18000, 20000, 22000, 24000)\n",
    "best = fit_until_k(X, target_k=10, ms_grid=(3, 5, 7, 9), mcs_grid=grid_lst)\n",
    "\n",
    "# 최적 파라미터 출력\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST 파라미터:\")\n",
    "print(f\"  - min_cluster_size: {best['mcs']}\")\n",
    "print(f\"  - min_samples: {best['ms']}\")\n",
    "print(f\"  - 클러스터 개수: {best['k']}\")\n",
    "print(f\"  - Noise 비율: {best['noise']:.1%}\")\n",
    "print(f\"  - Silhouette Score: {best['silhouette']:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clusters = best[\"labels\"]\n",
    "hdbscan_model = best[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 모델 및 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN 모델 저장\n",
    "joblib.dump(hdbscan_model, MODELS_DIR / \"hdbscan_model.joblib\")\n",
    "print(f\"✓ HDBSCAN 모델 저장: {MODELS_DIR / 'hdbscan_model.joblib'}\")\n",
    "\n",
    "# 클러스터 레이블 저장\n",
    "clusters_np = cp.asnumpy(clusters) if hasattr(clusters, 'get') else clusters\n",
    "np.save(OUTPUT_DIR / \"cluster_labels.npy\", clusters_np)\n",
    "\n",
    "print(f\"✓ 클러스터 레이블 저장: {OUTPUT_DIR / 'cluster_labels.npy'}\")\n",
    "print(f\"  - 총 샘플: {len(clusters_np):,}\")\n",
    "print(f\"  - 고유 클러스터: {len(np.unique(clusters_np))} (noise 포함)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 클러스터링 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy 배열로 변환\n",
    "X_np = cp.asnumpy(X) if hasattr(X, \"get\") else X\n",
    "\n",
    "# 평가 지표 출력\n",
    "evaluate_clustering(X_np, clusters_np, \"HDBSCAN\")\n",
    "\n",
    "# 클러스터별 샘플 수\n",
    "unique, counts = np.unique(clusters_np, return_counts=True)\n",
    "print(\"\\n[클러스터별 샘플 수]\")\n",
    "for cid, count in sorted(zip(unique, counts), key=lambda x: -x[1]):\n",
    "    label = \"Noise\" if cid == -1 else f\"Cluster {cid}\"\n",
    "    print(f\"  {label}: {count:>6,} ({count/len(clusters_np)*100:>5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 클러스터링 결과 시각화 (PCA 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA로 2차원으로 축소 (시각화용)\n",
    "Z = PCA(n_components=2).fit_transform(X_np)\n",
    "\n",
    "# DataFrame 생성\n",
    "df_temp = pd.DataFrame({\n",
    "    \"PC1\": Z[:, 0],\n",
    "    \"PC2\": Z[:, 1],\n",
    "    \"Cluster\": clusters_np\n",
    "})\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    data=df_temp, \n",
    "    x=\"PC1\", \n",
    "    y=\"PC2\", \n",
    "    hue=\"Cluster\", \n",
    "    s=6, \n",
    "    legend=\"full\",\n",
    "    palette=\"tab10\"\n",
    ")\n",
    "plt.title(\"HDBSCAN Clustering Result (PCA 2D Projection)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"PC1\", fontsize=12)\n",
    "plt.ylabel(\"PC2\", fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 클러스터링 결과 분석\n",
    "\n",
    "### TODO: Noise 데이터 처리 방안 검토\n",
    "\n",
    "현재 noise 비율이 ~38%로 높은 편입니다. 다음 방안들을 고려할 수 있습니다:\n",
    "\n",
    "1. **Soft Clustering**: 각 noise 포인트를 가장 가까운 클러스터에 할당\n",
    "2. **별도 분석**: Noise 그룹만 따로 2차 클러스터링 수행\n",
    "3. **분류 모델 학습**: 클러스터링 결과로 분류기를 학습한 후 noise 예측\n",
    "4. **임계값 조정**: `min_cluster_size`/`min_samples` 파라미터 재조정\n",
    "5. **그대로 유지**: Noise는 이상치로 간주하고 분석에서 제외\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1 클러스터 레이블 추가 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 DataFrame에 클러스터 레이블 추가\n",
    "df_cluster = df.with_columns(\n",
    "    pl.Series(\"cluster\", clusters_np)\n",
    ")\n",
    "\n",
    "# 클러스터별 개수 확인\n",
    "cluster_counts = (\n",
    "    df_cluster\n",
    "    .group_by(\"cluster\")\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"[클러스터별 샘플 수]\")\n",
    "print(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터링 결과 저장\n",
    "output_path = DATA_DIR / 'silver' / \"maude_clustered.parquet\"\n",
    "df_cluster.write_parquet(output_path)\n",
    "\n",
    "print(f\"✓ 클러스터링 결과 저장: {output_path}\")\n",
    "print(f\"  - 총 레코드: {df_cluster.shape[0]:,}\")\n",
    "print(f\"  - 컬럼 수: {df_cluster.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 클러스터별 샘플 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클러스터의 대표 샘플 확인\n",
    "for cid in sorted(df_cluster[\"cluster\"].unique()):\n",
    "    if cid == -1:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Cluster: NOISE (-1)\")\n",
    "        print(f\"{'='*60}\")\n",
    "    else:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Cluster {cid}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    samples = (\n",
    "        df_cluster\n",
    "        .filter(pl.col(\"cluster\") == cid)\n",
    "        .select(\"mdr_sntc\")\n",
    "        .head(3)\n",
    "    )\n",
    "    \n",
    "    for i, row in enumerate(samples.iter_rows(), 1):\n",
    "        print(f\"[{i}] {row[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 클러스터 대표 샘플 추출 (Centroid 기반)\n",
    "\n",
    "**TODO: Representative 선택 방법 개선 검토**\n",
    "\n",
    "현재는 Centroid(중심점)에 가장 가까운 샘플을 대표로 선택합니다.\n",
    "\n",
    "개선 방안:\n",
    "- **Medoid**: 클러스터 내 모든 포인트까지 거리 합이 최소인 실제 샘플\n",
    "- **Diversity Sampling**: Centroid 근처 + 경계 샘플 등 다양한 특성을 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representatives_polars(df_cluster, X_np, labels_np, n=10):\n",
    "    \"\"\"\n",
    "    클러스터별 대표 샘플 추출 (Centroid 기반)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_cluster : pl.DataFrame\n",
    "        클러스터 레이블이 포함된 DataFrame\n",
    "    X_np : np.ndarray\n",
    "        특징 벡터 (UMAP 결과)\n",
    "    labels_np : np.ndarray\n",
    "        클러스터 레이블\n",
    "    n : int\n",
    "        클러스터당 추출할 샘플 수\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {cluster_id: DataFrame} 형태의 대표 샘플들\n",
    "    \"\"\"\n",
    "    df_cluster_idx = df_cluster.with_row_index(\"row_id\")\n",
    "    reps = {}\n",
    "\n",
    "    for cid in np.unique(labels_np):\n",
    "        # Noise는 제외\n",
    "        if cid == -1:\n",
    "            continue\n",
    "\n",
    "        # 해당 클러스터의 인덱스 및 특징 벡터\n",
    "        idx = np.where(labels_np == cid)[0]\n",
    "        \n",
    "        # Centroid (중심점) 계산\n",
    "        centroid = X_np[idx].mean(axis=0)\n",
    "        \n",
    "        # Centroid까지의 거리 계산\n",
    "        dists = np.linalg.norm(X_np[idx] - centroid, axis=1)\n",
    "\n",
    "        # 거리가 가까운 순서대로 n개 선택\n",
    "        top_local = np.argsort(dists)[:n]\n",
    "        top_idx = idx[top_local]\n",
    "\n",
    "        # DataFrame 필터링\n",
    "        reps[cid] = (\n",
    "            df_cluster_idx\n",
    "            .filter(pl.col(\"row_id\").is_in(top_idx.tolist()))\n",
    "            .drop(\"row_id\")\n",
    "        )\n",
    "\n",
    "    return reps\n",
    "\n",
    "# 클러스터별 대표 샘플 5개씩 추출\n",
    "reps = get_representatives_polars(df_cluster, X_np, clusters_np, n=5)\n",
    "\n",
    "print(f\"✓ 클러스터별 대표 샘플 추출 완료 (각 {5}개)\")\n",
    "print(f\"  - 추출된 클러스터 수: {len(reps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대표 샘플 확인용 컬럼 정의\n",
    "MDR_COLS = [\n",
    "    'product_code', \n",
    "    'event_type', \n",
    "    'patient_harm', \n",
    "    'problem_components', \n",
    "    'defect_confirmed', \n",
    "    'defect_type', \n",
    "    'mdr_sntc'\n",
    "]\n",
    "\n",
    "# 예시: 클러스터 0의 대표 샘플\n",
    "if 0 in reps:\n",
    "    print(\"[Cluster 0 대표 샘플]\")\n",
    "    print(reps[0][MDR_COLS])\n",
    "else:\n",
    "    print(\"클러스터 0이 존재하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 클러스터별 특성 분석\n",
    "\n",
    "### 7.1 범주형 변수 분석 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_by_cluster(df_cluster, categorical_col, cluster_col='cluster', top_n=None):\n",
    "    \"\"\"\n",
    "    클러스터별 범주형 변수 비율 분석 및 시각화\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_cluster : pl.DataFrame\n",
    "        클러스터 레이블이 포함된 DataFrame\n",
    "    categorical_col : str\n",
    "        분석할 범주형 변수 컬럼명\n",
    "    cluster_col : str\n",
    "        클러스터 컬럼명 (기본값: 'cluster')\n",
    "    top_n : int, optional\n",
    "        상위 N개 범주만 분석 (None이면 전체)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (절대빈도 교차표, 비율 교차표)\n",
    "    \"\"\"\n",
    "    # Top N 범주 필터링\n",
    "    if top_n is not None:\n",
    "        top_categories = (\n",
    "            df_cluster\n",
    "            .group_by(categorical_col)\n",
    "            .agg(pl.len().alias('count'))\n",
    "            .sort('count', descending=True)\n",
    "            .head(top_n)\n",
    "            .select(categorical_col)\n",
    "            .to_series()\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "        df_filtered = df_cluster.filter(pl.col(categorical_col).is_in(top_categories))\n",
    "        print(f\"\\n상위 {top_n}개 범주 선택: {top_categories}\")\n",
    "    else:\n",
    "        df_filtered = df_cluster\n",
    "\n",
    "    # 1. 교차표 생성 (절대 빈도)\n",
    "    crosstab = (\n",
    "        df_filtered\n",
    "        .group_by([cluster_col, categorical_col])\n",
    "        .agg(pl.len().alias('count'))\n",
    "        .pivot(values='count', index=cluster_col, on=categorical_col)\n",
    "        .fill_null(0)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== {categorical_col} 절대 빈도 ===\")\n",
    "    print(crosstab)\n",
    "\n",
    "    # 2. 클러스터별 비율 계산\n",
    "    cluster_totals = (\n",
    "        df_cluster\n",
    "        .group_by(cluster_col)\n",
    "        .agg(pl.len().alias('total'))\n",
    "    )\n",
    "\n",
    "    crosstab_with_total = crosstab.join(cluster_totals, on=cluster_col)\n",
    "    cat_columns = [col for col in crosstab.columns if col != cluster_col]\n",
    "\n",
    "    crosstab_pct = crosstab_with_total.with_columns([\n",
    "        (pl.col(col) / pl.col('total') * 100).alias(f\"{col}_pct\")\n",
    "        for col in cat_columns\n",
    "    ]).select([cluster_col] + [f\"{col}_pct\" for col in cat_columns])\n",
    "\n",
    "    print(f\"\\n=== {categorical_col} 클러스터별 비율 (%) ===\")\n",
    "    print(crosstab_pct)\n",
    "\n",
    "    # 3. 시각화\n",
    "    crosstab_pd = crosstab.to_pandas().set_index(cluster_col)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    crosstab_pd.plot(kind='bar', stacked=False)\n",
    "    plt.title(f'클러스터별 {categorical_col} 분포', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Cluster', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.legend(title=categorical_col, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return crosstab, crosstab_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 이벤트 유형별 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_categorical_by_cluster(df_cluster, 'event_type', 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 환자 피해 여부별 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_categorical_by_cluster(df_cluster, 'patient_harm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 제품 코드별 분석 (Top 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_categorical_by_cluster(df_cluster, 'product_code', top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 문제 컴포넌트 키워드 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_keyword_unpack(df, col_name, cluster_col='cluster'):\n",
    "    \"\"\"\n",
    "    클러스터별 키워드 빈도 분석 (리스트 타입 컬럼)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        분석 대상 DataFrame\n",
    "    col_name : str\n",
    "        리스트가 들어있는 컬럼명 (예: 'problem_components')\n",
    "    cluster_col : str\n",
    "        클러스터 컬럼명 (기본값: 'cluster')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        클러스터별 키워드 빈도 Counter 객체 리스트\n",
    "    \"\"\"\n",
    "    df_temp = df.select([cluster_col, col_name])\n",
    "\n",
    "    # 문자열을 리스트로 변환 (필요한 경우)\n",
    "    if df_temp[col_name].dtype == pl.Utf8:\n",
    "        df_temp = df_temp.with_columns(\n",
    "            pl.col(col_name)\n",
    "            .map_elements(lambda x: ast.literal_eval(x) if x else [], return_dtype=pl.List(pl.Utf8))\n",
    "        )\n",
    "\n",
    "    # 리스트를 explode하여 키워드별로 분리\n",
    "    exploded_df = (\n",
    "        df_temp\n",
    "        .explode(col_name)\n",
    "        .filter(pl.col(col_name).is_not_null())\n",
    "        .filter(pl.col(col_name) != \"\")\n",
    "    )\n",
    "\n",
    "    # 클러스터별 키워드 빈도 계산\n",
    "    keyword_counts = (\n",
    "        exploded_df\n",
    "        .with_columns(\n",
    "            pl.col(col_name).str.to_lowercase().str.strip_chars()\n",
    "        )\n",
    "        .group_by([cluster_col, col_name])\n",
    "        .agg(pl.len().alias('count'))\n",
    "        .sort([cluster_col, 'count'], descending=[False, True])\n",
    "    )\n",
    "\n",
    "    # 클러스터별로 Counter 생성\n",
    "    unique_clusters = df[cluster_col].unique().sort()\n",
    "    cluster_lst = []\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_data = keyword_counts.filter(pl.col(cluster_col) == cluster_id)\n",
    "        counts = Counter(dict(zip(\n",
    "            cluster_data[col_name].to_list(),\n",
    "            cluster_data['count'].to_list()\n",
    "        )))\n",
    "        cluster_lst.append(counts)\n",
    "\n",
    "    # 결과 출력\n",
    "    for i, counts in enumerate(cluster_lst):\n",
    "        cid = unique_clusters[i]\n",
    "        label = \"NOISE\" if cid == -1 else f\"Cluster {cid}\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{label}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"총 키워드 수: {sum(counts.values()):,}\")\n",
    "        print(f\"고유 키워드 수: {len(counts):,}\")\n",
    "        print(f\"\\nTop 10 키워드:\")\n",
    "        for keyword, count in counts.most_common(10):\n",
    "            print(f\"  {keyword:30s}: {count:>6,}\")\n",
    "\n",
    "    return cluster_lst\n",
    "\n",
    "# 문제 컴포넌트 키워드 분석\n",
    "keyword_result = cluster_keyword_unpack(df_cluster, 'problem_components', 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 결과 요약\n",
    "\n",
    "### 저장된 파일\n",
    "\n",
    "1. **모델 파일** (`models/`)\n",
    "   - `umap_model.joblib`: UMAP 차원 축소 모델\n",
    "   - `hdbscan_model.joblib`: HDBSCAN 클러스터링 모델\n",
    "\n",
    "2. **중간 결과** (`output/`)\n",
    "   - `embeddings.npy`: SentenceTransformer 임베딩 (768D)\n",
    "   - `umap_X.npy`: UMAP 축소 결과 (15D)\n",
    "   - `cluster_labels.npy`: 클러스터 레이블\n",
    "\n",
    "3. **최종 결과** (`data/silver/`)\n",
    "   - `maude_clustered.parquet`: 클러스터 레이블이 추가된 전체 데이터\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "- [ ] Noise 데이터 처리 방안 결정\n",
    "- [ ] Representative 샘플 선택 방법 개선\n",
    "- [ ] 클러스터별 라벨링 (의미 부여)\n",
    "- [ ] 통계 분석 및 인사이트 도출"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
